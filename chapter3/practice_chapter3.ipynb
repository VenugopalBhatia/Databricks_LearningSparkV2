{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as s,functions as fcn\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (s\n",
    "        .builder\n",
    "        .appName(\"chapter3\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 19:39:08 WARN Utils: Your hostname, Venugopals-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.220.234.156 instead (on interface en0)\n",
      "25/02/23 19:39:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/23 19:39:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD = (sc.parallelize(c=[(\"Brooke\",20),(\"Danny\",21),(\"Jules\",20),(\"Charles\",30),(\"Brooke\",25)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pyspark_practice/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=22306) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Brooke', 22.5, 2),\n",
       " ('Danny', 21.0, 1),\n",
       " ('Charles', 30.0, 1),\n",
       " ('Jules', 20.0, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 19:39:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "agesRDD = (dataRDD.map(lambda x:(x[0],(x[1],1)))\n",
    "            .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "            .map(lambda x: (x[0],x[1][0]/x[1][1],x[1][1]))\n",
    "        )\n",
    "\n",
    "agesRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(dataRDD,[\"name\",\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "| Brooke| 20|\n",
      "|  Danny| 21|\n",
      "|  Jules| 20|\n",
      "|Charles| 30|\n",
      "| Brooke| 25|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_df = (df\n",
    "           .select(\"name\",\"age\")\n",
    "           .groupBy(\"name\")\n",
    "           .agg(fcn.mean(\"age\").alias(\"averageAge\"),fcn.count(\"age\").alias(\"countPeople\"))\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|   name|averageAge|countPeople|\n",
      "+-------+----------+-----------+\n",
      "| Brooke|      22.5|          2|\n",
      "|  Danny|      21.0|          1|\n",
      "|  Jules|      20.0|          1|\n",
      "|Charles|      30.0|          1|\n",
      "+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ages_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "ages_sql_query = (spark.sql('''SELECT \n",
    "                                    NAME, \n",
    "                                    MEAN(AGE),COUNT(*) AS AVERAGE_AGE \n",
    "                                FROM DF \n",
    "                                GROUP BY NAME''')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+\n",
      "|   NAME|mean(AGE)|AVERAGE_AGE|\n",
      "+-------+---------+-----------+\n",
      "| Brooke|     22.5|          2|\n",
      "|  Danny|     21.0|          1|\n",
      "|  Jules|     20.0|          1|\n",
      "|Charles|     30.0|          1|\n",
      "+-------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ages_sql_query.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
